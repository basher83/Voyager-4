# Evaluations Directory

This directory contains tools and frameworks for testing and measuring Claude Code prompt effectiveness. The evaluation system follows Anthropic's recommended practices for empirical testing.

## ðŸ“ Structure

### scripts/
Automated evaluation tools and testing pipelines:
- **Evaluation runners** for different prompt types
- **Comparison tools** for A/B testing prompts
- **Metric calculators** for performance analysis
- **Report generators** for results visualization

### results/
Stored evaluation outcomes and analysis:
- **Test runs** with timestamps and configurations
- **Comparison reports** between prompt variants
- **Performance metrics** over time
- **Failure analysis** and edge case documentation

### metrics/
Performance measurement frameworks:
- **Task fidelity** metrics (accuracy, completeness)
- **Consistency** measurements (cosine similarity)
- **Quality** assessments (LLM-based grading)
- **Performance** tracking (latency, cost)

## ðŸŽ¯ Evaluation Framework

Based on [Anthropic's evaluation guidelines](../anthropic-md/en/docs/test-and-evaluate/develop-tests.md), our framework includes:

### 1. Task-Specific Evaluations
**Mirrors real-world task distribution:**
- Representative test cases
- Edge case scenarios
- Production-like conditions
- Diverse input variations

### 2. Automated Grading
**Structured for scalability:**
- **Code-based**: Exact match, string matching, regex
- **LLM-based**: Quality assessment, rubric evaluation
- **Metric-based**: ROUGE scores, cosine similarity
- **Custom**: Domain-specific measurements

### 3. Volume Over Perfection
**Prioritizes comprehensive testing:**
- Many automated tests over few manual ones
- Statistical significance through volume
- Rapid iteration and feedback
- Continuous improvement

## ðŸ“Š Evaluation Methods

### Exact Match Evaluation
```python
# For categorical outputs (sentiment, classification)
def evaluate_exact_match(model_output, correct_answer):
    return model_output.strip().lower() == correct_answer.lower()
```

### Cosine Similarity Evaluation
```python
# For consistency testing across similar inputs
def evaluate_cosine_similarity(outputs):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = [model.encode(output) for output in outputs]
    return np.mean(cosine_similarities)
```

### LLM-Based Evaluation
```python
# For subjective quality assessment
def evaluate_quality(model_output, rubric):
    grader_prompt = f"Grade this output based on: {rubric}"
    return llm_grade(grader_prompt, model_output)
```

### Custom Metrics
```python
# For domain-specific requirements
def evaluate_code_quality(code_output):
    return {
        'syntax_valid': check_syntax(code_output),
        'follows_style': check_style_guide(code_output),
        'has_tests': contains_tests(code_output)
    }
```

## ðŸ”¬ Testing Pipeline

### 1. Test Case Generation
- **Real-world scenarios** from actual use cases
- **Synthetic data** generated by Claude
- **Edge cases** for robustness testing
- **Failure modes** for boundary testing

### 2. Prompt Comparison
- **A/B testing** between prompt variants
- **Performance benchmarking** across metrics
- **Statistical analysis** for significance
- **Cost-benefit analysis** for optimization

### 3. Results Analysis
- **Success rate** calculation
- **Failure mode** identification
- **Performance trends** over time
- **Improvement recommendations**

## ðŸ“‹ Success Criteria

Following [Anthropic's success criteria framework](../anthropic-md/en/docs/test-and-evaluate/define-success.md):

### Task Fidelity
- **Accuracy**: Correctness of outputs
- **Completeness**: Coverage of requirements
- **Relevance**: Alignment with user needs
- **Edge case handling**: Robustness

### Consistency
- **Similar inputs**: Comparable outputs
- **Semantic similarity**: Content alignment
- **Format consistency**: Structure reliability
- **Behavioral predictability**: Stable patterns

### Quality Attributes
- **Tone and style**: Appropriate communication
- **Context utilization**: Effective use of information
- **Privacy preservation**: Sensitive data handling
- **Performance**: Speed and efficiency

## ðŸ› ï¸ Evaluation Tools

### Quick Start Scripts
```bash
# Run basic evaluation
python scripts/evaluate_prompt.py --prompt base/codebase-overview.md --test_cases test_cases/real-world/

# Compare prompts
python scripts/compare_prompts.py --baseline base/ --variant enhanced/ --metrics accuracy,consistency

# Generate report
python scripts/generate_report.py --results results/latest/ --format html
```

### Configuration
```yaml
# evaluation_config.yaml
evaluation:
  metrics: [accuracy, consistency, quality]
  test_cases: test_cases/real-world/
  iterations: 100
  confidence_level: 0.95
  
prompts:
  baseline: templates/base/
  variants: [templates/enhanced/, templates/advanced/]
```

## ðŸ“ˆ Metrics Dashboard

Key performance indicators tracked:

| Metric | Target | Current | Trend |
|--------|--------|---------|-------|
| Task Accuracy | >85% | - | - |
| Consistency Score | >0.8 | - | - |
| Avg Response Time | <2s | - | - |
| Cost per Query | <$0.05 | - | - |

## ðŸ”„ Continuous Improvement

### Evaluation Loop
1. **Design tests** based on success criteria
2. **Run evaluations** on prompt variants
3. **Analyze results** and identify patterns
4. **Refine prompts** based on insights
5. **Re-evaluate** and validate improvements

### Best Practices
- **Regular evaluation** of production prompts
- **Version control** for reproducibility
- **Documentation** of findings and decisions
- **Collaboration** with domain experts

## ðŸ”— Related Resources

- [Define Success Criteria](../anthropic-md/en/docs/test-and-evaluate/define-success.md)
- [Develop Tests](../anthropic-md/en/docs/test-and-evaluate/develop-tests.md)
- [Prompt Engineering Overview](../anthropic-md/en/docs/build-with-claude/prompt-engineering/overview.md)
- [Templates Documentation](../templates/README.md)

---

*Evaluation framework designed for systematic prompt optimization using data-driven insights and proven testing methodologies.*